{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seanabuklau/LLM_Document_Summariser/blob/main/COR_LAW_2215.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKrlQ_HMJyOW"
      },
      "source": [
        "**1. Install pre-requisite libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "winK5txoCSEE",
        "outputId": "ccf87557-4d65-4c32-af33-67821be19aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: shapely==1.8.5 in /usr/local/lib/python3.10/dist-packages (1.8.5)\n"
          ]
        }
      ],
      "source": [
        "# Install requisite libraries\n",
        "!pip install spacy -q\n",
        "!pip install nltk -q\n",
        "!pip install langchain -q\n",
        "!pip install \"shapely==1.8.5\"\n",
        "!pip install google-cloud-aiplatform --upgrade -q\n",
        "!pip install tiktoken -q\n",
        "!pip install transformers -q\n",
        "!pip install PyPDF2 -q\n",
        "!pip install scikit-learn -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJwal6DCE9Rb"
      },
      "source": [
        "**2. Pre-Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2LgBBg_YIki"
      },
      "source": [
        "*Step 1: Parse Judgement Case*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNivoHAUUXHz"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "pdf_file_path = \"\"\n",
        "extracted_text = \"\"\n",
        "\n",
        "with open(pdf_file_path, \"rb\") as pdf_file:\n",
        "    reader = PyPDF2.PdfReader(pdf_file)\n",
        "    num_pages = len(reader.pages)\n",
        "    for page_number in range(num_pages):\n",
        "        page = reader.pages[page_number]\n",
        "        extracted_text += page.extract_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7OGUbjzfZjW"
      },
      "source": [
        "*Step 2: Sentence Tokenization*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ujx6Yfcg7dS"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(extracted_text)\n",
        "\n",
        "# Extract sentences from the processed text\n",
        "sentences = [sent.text for sent in doc.sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1QhwNpzfe0F"
      },
      "source": [
        "*Step 3: Sentence Cleansing*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvsVmWxenI79"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_sentences(sentences):\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Remove special characters and extra whitespace\n",
        "        clean_sentence = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", sentence)\n",
        "        clean_sentence = clean_sentence.strip()\n",
        "        if clean_sentence:\n",
        "            cleaned_sentences.append(clean_sentence)\n",
        "    return cleaned_sentences\n",
        "\n",
        "cleaned_sentences = clean_sentences(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU7eRWYHfihK"
      },
      "source": [
        "*Step 4: Stop Word Removal and Lemmitization*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBVS0YvIouBQ",
        "outputId": "5dfe0e91-540b-44a0-e905-e232515c973f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(cleaned_sentences):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    word_list = [sentence.lower().split() for sentence in cleaned_sentences]\n",
        "    ans = [lemmatizer.lemmatize(word) for words in word_list for word in words if word not in stop_words]\n",
        "    return \" \".join(ans)\n",
        "\n",
        "preprocessed_text = preprocess_text(cleaned_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4PoBPVdFEBa"
      },
      "source": [
        "**3. In-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2c6HUgUFH7r"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "import vertexai\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain, LLMChain, StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "# Authentication\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()\n",
        "vertexai.init(project=\"GCP_PROJECT_ID\", location=\"asia-southeast1\")\n",
        "\n",
        "# LLM Selection\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.3,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Initialise Map Objects\n",
        "map_template = \"\"\"The following is a court judgement case:\n",
        "{preprocessed_text}\n",
        "Based on this, please summarise the case as much as possible while ensuring it is as verbose as possible.\n",
        "Include the case facts, issue(s), court's ruling and explanation, laws used and the case outcome\n",
        "\"\"\"\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "# Initialise Reduce Objects\n",
        "reduce_template = \"\"\"The following is a set of summaries:\n",
        "{doc_summaries}\n",
        "Take these and distill it into a final, consolidated summary.\n",
        "Include the case facts, issue(s), court's ruling and explanation, laws used and the case outcome\n",
        "\"\"\"\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Initialise StuffDocumentsChain:\n",
        "# -> This will take a list of documents, combine them into a single string, and passes this to an LLMChain\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
        ")\n",
        "\n",
        "# Initialise Iterator Reducer\n",
        "# -> Combines and iteravely reduces the mapped documents with the help of StuffDocumentsChain\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=8000,\n",
        ")\n",
        "\n",
        "# Initialise Map Reduce Chain\n",
        "# -> Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"preprocessed_text\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        ")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=20000, chunk_overlap=10)\n",
        "chunks = text_splitter.create_documents([preprocessed_text])\n",
        "\n",
        "# Intiatiate Map Reduce Chain\n",
        "summary = map_reduce_chain.run(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRRxnU934Dmt"
      },
      "source": [
        "**4. Post-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqNyvM36Iy9K"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "doc = [Document(page_content=summary, metadata={\"source\": \"local\"})]\n",
        "\n",
        "refine_template = (\n",
        "    \"Your job is to produce a final summary\\n\"\n",
        "    \"We have provided an existing summary up to a certain point:\\n\"\n",
        "    \"{summary}\"\n",
        "    \"\\nYou have the opportunity to refine the existing summary\"\n",
        ")\n",
        "refine_prompt = PromptTemplate.from_template(refine_template)\n",
        "\n",
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",\n",
        "    refine_prompt=refine_prompt,\n",
        "    input_key=\"input_documents\",\n",
        "    output_key=\"output_text\",\n",
        ")\n",
        "\n",
        "result = chain({\"input_documents\": doc}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQtDdbb71KXS"
      },
      "source": [
        "**5. Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q_qZXuAIkUx"
      },
      "source": [
        "1. Jaccard Similarity Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "571dd3Vf1LxP",
        "outputId": "e65b560e-7c4b-41c6-ef30-0e510560396a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jaccard Similarity: 0.12\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Parse Benchmark text\n",
        "benchmark_text = \"\"\n",
        "with open(\"[COMPARE] sample_law_case_3_updated.txt\", \"r\") as f:\n",
        "    benchmark_text = f.read()\n",
        "    f.close()\n",
        "\n",
        "\n",
        "# Tokenize the text into words or tokens\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "tokens1 = set(vectorizer.build_analyzer()(summary))\n",
        "tokens2 = set(vectorizer.build_analyzer()(benchmark_text))\n",
        "\n",
        "# Calculate the Jaccard similarity score\n",
        "jaccard_similarity = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
        "\n",
        "print(f\"Jaccard Similarity: {jaccard_similarity:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO_6hrNoItBY"
      },
      "source": [
        "2. Cosine Similarity Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p55OJUK2Iwzm",
        "outputId": "aa64d3ad-c2af-4c1a-fe5b-55e057047e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity between summary and benchmark_text: 0.75\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents to TF-IDF vectors\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([summary, benchmark_text])\n",
        "\n",
        "# Calculate cosine similarity between the documents\n",
        "cosine_similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
        "\n",
        "print(f\"Cosine Similarity between summary and benchmark_text: {cosine_similarity_score[0][0]:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
